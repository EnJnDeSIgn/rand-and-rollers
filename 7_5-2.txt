The Magical Number Seven, Plus or Minus Two
George A. Miller’s landmark 1956 paper proposed that the capacity of short-term (working) memory is about seven “chunks” of information, give or take two.
Information Capacity in Bits
Miller observed a parallel with one-dimensional absolute-judgment tasks, where people can distinguish roughly four to eight levels of a stimulus—about 2 to 3 bits of information per decision.
Chunks: The Building Blocks of Memory
- A chunk is the largest meaningful unit recognizable to the individual.
- Chunks vary in complexity: binary digits (1 bit each), decimal digits (~3.32 bits each), words (~10 bits each).
- Simon and Chase’s chunking strategy shows how reorganizing information into meaningful patterns can dramatically boost recall.
Beyond Miller’s Rule
Subsequent research suggests true working memory capacity often hovers closer to four chunks in many experimental tasks. Factors such as domain expertise, rehearsal strategies, and cognitive load all influence how much we can hold in mind at once.

Toward a Mathematical Model of “7 ± 2”
We can translate Miller’s rule into a bit‐based capacity constraint and then express how many chunks fit into working memory.

1. Total‐Bits Constraint
Short-term (working) memory seems to hold a nearly fixed amount of information, often estimated around B ≃ 25 bits.  If you parcel that into N chunks, each of average size hᵢ (in bits), you get:
\sum_{i=1}^{N} h_i \;\le\; B
When all chunks are roughly the same size (hᵢ = h), this simplifies to:
N \;\le\; \frac{B}{h}

2. Relating to “7 ± 2”
Miller’s “7” arises when you assume a typical chunk is about h ≈ 3.5 bits:
- If B=25 bits and h=3.5 bits, then
N = \frac{25}{3.5} \approx 7.1
- The ±2 spread reflects natural variability in B and h across tasks and individuals.

3. A Probabilistic Retrieval Function
Actual recall isn’t a hard cutoff at N = 7; accuracy gradually falls off.  A convenient sigmoid form is:
P_{\text{correct}}(N)\;=\; \frac{1}{1 \;+\;\exp\!\bigl(\tfrac{N - \mu}{\lambda}\bigr)}
- μ ≈ 7 is the inflection point
- λ controls how steeply performance drops (λ ≈ 1 gives ±2 as the 5–95 % bounds)
Or, equivalently, a Gaussian‐like model:
P_{\text{correct}}(N)\;=\;\exp\!\Bigl(-\tfrac{(N-\mu)^2}{2\sigma^2}\Bigr)
with σ ≈ 2.

4. Putting It All Together
- Define your chunks: estimate h (bits) per item (e.g. digits ≈ 3.3 bits, letters ≈ 4.7 bits, words ≈ 10+ bits).
- Compute theoretical N-max:
N_{\max} = \bigl\lfloor B/h\bigr\rfloor
- Predict recall probability: plug N into one of the functions above to see how accuracy decays beyond the mean.

What’s Next?
- Dynamic chunking: let h shrink as you group items into hierarchies—models of expertise.
- Time‐dependent decay: introduce a time‐constant τ so bits leak out, adding e^{-t/τ} factors.
- Neural implementation: map B to neural firing‐rate limits or synaptic states.
Which direction sounds most intriguing to you?

A Compact Formula for “7 ± 2” in Bits-Based Memory
Below is a streamlined mathematical translation of Miller’s rule. It treats working‐memory capacity as a finite bit-resource and shows how variability in that resource yields the familiar ±2 spread.

1. Core Capacity Equation
Let
• B = total working-memory capacity in bits (mean ≈ 25 bits)
• h = average size of one chunk in bits
Then the mean number of chunks you can hold is
\mu_N = \frac{B}{\mu_h}
If you assume fixed-size chunks (hᵢ = h), this reduces to
N_{\max} \;=\;\bigl\lfloor \tfrac{B}{h}\bigr\rfloor

2. Modeling Variability: Why “±2”?
Memory capacity and chunk‐size both vary from person to person and task to task.  If B and h are random variables with standard deviations σ_B and σ_h, then by propagation of uncertainty, the standard deviation of N is approximately:
\sigma_N \approx


\sqrt{\!\bigl(\tfrac{\sigma_B}{\mu_h}\bigr)^2


\;+\;\bigl(\tfrac{\mu_B\,\sigma_h}{\mu_h^2}\bigr)^2}
Plugging in typical values
• μ_B ≈ 25 bits, σ_B ≈ 3 bits
• μ_h ≈ 3.5 bits, σ_h ≈ 0.5 bits
gives
\sigma_N \approx


\sqrt{\!(3/3.5)^2 + (25·0.5/3.5^2)^2}


\;\approx\;2
hence the familiar “7 ± 2.”

3. Putting It All Together
- Estimate chunk-size
| Item           | h (bits)  | |----------------|-----------| | Binary digit   | 1         | | Decimal digit  | 3.32      | | Letter (A–Z)   | 4.7       | | Common word    | 10–12     |
- Compute μ_N and σ_N via the formulas above.
- Predict recall with a Gaussian approximation:
P_{\text{correct}}(n)


= \exp\!\Bigl(-\tfrac{(n-\mu_N)^2}{2\,\sigma_N^2}\Bigr)

4. Extensions to Explore Next
- Time-Decay: let B→B·e⁻ᵗ/τ and see how μ_N falls off with retention interval.
- Hierarchical Chunking: model h as a function of expertise (h↓ with practice).
- Neural Noise: link B to signal-to-noise constraints in spiking‐rate models.
Which of these extensions would you like to dive into?

Choosing an Extension to Explore
Which of the following directions should we dive into next?
- Time-Decay Model
Introduce a retention-interval τ so that capacity B(t)=B·e⁻ᵗ/τ, then derive how μ_N and recall curves change over time.
- Hierarchical Chunking
Let practice shrink average chunk-size h as expertise grows—model h(k)=h₀·k^–α—and predict how N_max increases with learning.
- Neural-Noise Constraints
Link total bits B to biophysical limits (e.g., signal-to-noise in spiking neurons) and see how synaptic noise shapes σ_N.

Bonus Idea: Context-Dependent Capacity
We could also factor in arousal or multitasking by making B a function of cognitive load L, e.g. B(L)=B₀·(1–βL), to predict when memory collapses under distraction.

